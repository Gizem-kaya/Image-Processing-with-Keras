{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Improving_Deep_Convolutional_Networks.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpMyZu8z76hy",
        "colab_type": "text"
      },
      "source": [
        "# **Improving Deep Convolutional Networks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5LGcoHN9L35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Dense, Flatten, Dropout\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlwgiSs278_Z",
        "colab_type": "text"
      },
      "source": [
        "## **Tracking learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gecxct9A-hZU",
        "colab_type": "text"
      },
      "source": [
        "* Plotting the learning and validation loss curves for a model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZE6toO87vQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training = model.fit(train_data, train_labels, validation_split = 0.2, epochs = 3, batch_size = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5Vi0VqO-2Mt",
        "colab_type": "text"
      },
      "source": [
        "Training the model and store the training object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDaCje_--y7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = training.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xW4MHN6-2b4",
        "colab_type": "text"
      },
      "source": [
        "Extracting the history from the training object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNLJiFww-0Hz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history['loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au95hZ-p_EmD",
        "colab_type": "text"
      },
      "source": [
        "Plotting the training loss and validation loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzD9GAGu_-Jp",
        "colab_type": "text"
      },
      "source": [
        "Model weights stored in an hdf5 file can be reused to populate an untrained model. Once the weights are loaded into this model, it behaves just like a model that has been trained to reach these weights. To use stored weights to predict in a test set :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1t694yyAHfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('weights.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plAsSJCyAhn0",
        "colab_type": "text"
      },
      "source": [
        "Loading the weights from file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsmtq_76AgdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(test_data[0:3]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP8AV5PWAljW",
        "colab_type": "text"
      },
      "source": [
        "Predicting from the first three images in a test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pqrkx5CDp7e",
        "colab_type": "text"
      },
      "source": [
        "## **Regularization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKsyUsCcDyIg",
        "colab_type": "text"
      },
      "source": [
        "1. Dropout\n",
        "2. Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8qv14G5D6-w",
        "colab_type": "text"
      },
      "source": [
        "**1. Dropout :** \n",
        "Select a subset ofthe units, ignore it in the forward pass and in the back-propagation of error. Dropout slows down learning, making the learning incremental and careful. If we want to add a dropout to our neural network we should add a dropout layer : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MZ6NCjVD3bs",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "model.add(Conv2D(15, kernel_size=2, activation='relu', \n",
        "                 input_shape=(img_rows, img_cols, 1)))\n",
        "\n",
        "\n",
        "model.add(Dropout(0.2))   # A dropout layer addition.\n",
        "\n",
        "\n",
        "model.add(Conv2D(5, kernel_size=2, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBX7RBPEEBHZ",
        "colab_type": "text"
      },
      "source": [
        "**2. Batch Normalization :**\n",
        "Rescale the outputs. Batch normalization tends to make learning go faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5p_v5-OGYb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Conv2D(15, kernel_size = 2, activation = 'relu', input_shape=(img_rows, img_cols, 1)))\n",
        "\n",
        "model.add(BatchNormalization())   # Batch normalization layer addition.\n",
        "\n",
        "model.add(Conv2D(5, kernel_size=2, activation='relu'))\n",
        "model.add(Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wc3VuHEHks4",
        "colab_type": "text"
      },
      "source": [
        "## **Interpreting The Model**\n",
        "\n",
        "### **Extracting a kernel from a trained network**\n",
        "One way to interpret models is to examine the properties of the kernels in the convolutional layers. Down below, we can see the extraction one of the kernels from a convolutional neural network with weights that it is saved in a hdf5 file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1YZ9JDbJU-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('weights.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kE7hDR1JdL9",
        "colab_type": "text"
      },
      "source": [
        "Loading the weights into the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-dmTwwmJWs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c1 = model.layers[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAmZTLwXJhWb",
        "colab_type": "text"
      },
      "source": [
        "Getting the first convolutional layer from the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNZf-1YdJYBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights1 = c1.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMdHN7kOJkca",
        "colab_type": "text"
      },
      "source": [
        "Getting the weights of the first convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsPRhMyaJZxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel = weights1[0][...,0, 0]\n",
        "print(kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc_1Il7xJn2k",
        "colab_type": "text"
      },
      "source": [
        "Pull out the first channel of the first kernel in the first layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuT2fruCKkUP",
        "colab_type": "text"
      },
      "source": [
        "**Visualizing kernel responses**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTWHFjHwKfwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = convolution(test_data[3, :, :, 0], kernel)\n",
        "\n",
        "plt.imshow(out)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHOXTFS3KnwX",
        "colab_type": "text"
      },
      "source": [
        "Convolving with the fourth image in test_data."
      ]
    }
  ]
}